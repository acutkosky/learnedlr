model:
  embedding_dim: 768
  n_layers: 12
  n_heads: 4
  context_length: 10
  use_diag: False
  use_fc: True
train:
  framework: 'pytorch'
  optimizer: custom # current options: adamw, pertensor_randomol

  context_length: 10


  custom_opt:

    name: 'optax_rand_scaled'
    ogd:
      lr: 0.01
    
    cb:
      eps: 1e-8
      eta: 2.219 #(2/(2 - log(3)))
      decay: 0.99


    optax_learned_lr_ada:
      optax_optimizer: 'adamw'
      optax_args: []
      optax_kwargs:
        learning_rate: 1.0
        b1: 0.9
        b2: 0.99
        weight_decay: 0.0001
      ol_init: 'adagrad_init'
      ol_update_fn: 'adagrad_update'
      ol_args: []
      ol_kwargs:
        eps: 1e-8
        lr: 0.0001
        decay: 0.999
      lower_bound: 1e-3
      upper_bound: 100.0
      multiply: true
      additive_bounds: false
      clip: 0.01

    adamw_learned_lr:
      beta1: 0.9
      beta2: 0.99
      wd: 0.0001
      lower_bound: 1e-3
      upper_bound: 100.0
      cb:
        eps: 1e-5
        eta: 2.219
        decay: 1.0

    ol_momentum:
      ol_init: 'cb_stable_init'
      ol_update_fn: 'cb_stable_update'
      ol_reset_fn: 'cb_stable_reset'
      ol_args: []
      ol_kwargs:
        eps: 1e-4
        eta: 2.219
        decay: 0.999
      reset_threshold: 100.0


    ol_momentum_2:
      ol_init: 'adagrad_init'
      ol_update_fn: 'adagrad_update'
      ol_reset_fn: 'adagrad_reset'
      ol_args: []
      ol_kwargs:
        lr: 1e-4
        decay: 0.9
      reset_threshold: 100.0




    optax_learned_lr:
      optax_optimizer: 'adamw'
      optax_args: []
      optax_kwargs:
        learning_rate: 1.0
        b1: 0.9
        b2: 0.99
        weight_decay: 0.0001
      ol_init: 'cb_stable_init'
      ol_update_fn: 'cb_stable_update'
      ol_args: []
      ol_kwargs:
        eps: 1e-3 #1e-8
        eta: 1.0
        decay: 0.9999
        stability: 0.0
        grad_stab: 2.0
      lower_bound: 0.0 #0.0 # 1e-3
      upper_bound: 1e-2 #10.0
      steps_per_ol_update: 1 #25
      multiply: false
      additive_bounds: true
      use_loss_diff: false
      use_rand_scaling: true
      per_variable_lr: true
      clip: 0.1
      clip_meta_grad: 200.0
      


    optax_rand_scaled:
      optax_optimizer: 'adamw'
      optax_args: []
      optax_kwargs:
        learning_rate: 1.0
        b1: 0.9
        b2: 0.99
        weight_decay: 0.0001
      use_loss_diff: true
      rand_scaling_type: none
      clip: 1000.0
      

  wd: 0.0001
  clip: 0.01

  # generic options useful for all training algorithms
  warmup_examples: 204800 # how many examples of linear warmup to perform
  total_examples: 12288000
  lr: 0.001 #0.001 #0.001 # learning rate
  beta1: 0.9 #beta values for adam algorithms
  beta2: 0.99

  linear_decay: False
  cosine_decay: False
  true_cosine_decay: False

  decay_type: none # linear

  # generic options for the trainer
  num_workers: 3
  batch_size: 1024
  valid_frequency_examples: 409600 # how many training examples to process between computing validation performance (training set is too big to actually pass all the way through even once)
  valid_examples: 51200 # how many examples to use for validation (validation set is pretty big, so we just subsample by looping through it when computing validation stats)


  # options for wandb
  log_differences: false
  logging: true
  wandb_project: C4_adamw_jax_12M_debug
  log_interval: 5